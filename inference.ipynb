{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9551fe6d",
   "metadata": {
    "papermill": {
     "duration": 0.005819,
     "end_time": "2023-03-18T03:42:37.350600",
     "exception": false,
     "start_time": "2023-03-18T03:42:37.344781",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " # **Installing & Importing dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c2f31f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-18T03:42:37.362159Z",
     "iopub.status.busy": "2023-03-18T03:42:37.361171Z",
     "iopub.status.idle": "2023-03-18T03:43:01.701361Z",
     "shell.execute_reply": "2023-03-18T03:43:01.699956Z"
    },
    "papermill": {
     "duration": 24.349412,
     "end_time": "2023-03-18T03:43:01.704509",
     "exception": false,
     "start_time": "2023-03-18T03:42:37.355097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install the tqdm package using pip\n",
    "!pip install tqdm\n",
    "import Levenshtein\n",
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from tqdm import tqdm # Import tqdm to show progress bar during loops\n",
    "import re # Import regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9571d",
   "metadata": {
    "papermill": {
     "duration": 0.003821,
     "end_time": "2023-03-18T03:43:01.712607",
     "exception": false,
     "start_time": "2023-03-18T03:43:01.708786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference(Takes about 20 mins for 5000 samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87393b6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T03:43:01.722804Z",
     "iopub.status.busy": "2023-03-18T03:43:01.722331Z",
     "iopub.status.idle": "2023-03-18T03:43:01.859257Z",
     "shell.execute_reply": "2023-03-18T03:43:01.858023Z"
    },
    "papermill": {
     "duration": 0.145794,
     "end_time": "2023-03-18T03:43:01.862503",
     "exception": false,
     "start_time": "2023-03-18T03:43:01.716709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Just change the path of the csv in the dataframe test_data for running inference on hidden dataset. \n",
    "Please kindly make sure that the csv file has a column named \"text\" as it explicitly called in the \n",
    "inference and post processing cell. If the csv file has any other name then kindly change the column name\n",
    "in the line number 25 of this cell and line number 83, 88, 112, 114, and 136.\n",
    "\"\"\"\n",
    "# Read the test dataset\n",
    "test_data = pd.read_csv(\"/kaggle/input/bhashabhrom-evaluation-thirdset/thirdset.csv\")\n",
    "test_data['sentence'] = test_data['sentence'].apply(lambda x: x.replace('$',''))\n",
    "test_data[\"Expected\"] = test_data['sentence'] \n",
    "test_data[\"text\"] = test_data['sentence'] \n",
    "# Create an empty dataframe with columns 'Id' and 'Expected'\n",
    "df = pd.DataFrame(columns=['Id', 'Expected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7658c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T03:43:01.873500Z",
     "iopub.status.busy": "2023-03-18T03:43:01.873094Z",
     "iopub.status.idle": "2023-03-18T04:29:15.895196Z",
     "shell.execute_reply": "2023-03-18T04:29:15.893725Z"
    },
    "papermill": {
     "duration": 2774.031098,
     "end_time": "2023-03-18T04:29:15.898284",
     "exception": false,
     "start_time": "2023-03-18T03:43:01.867186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [45:57<00:00,  3.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the T5 tokenizer and model using a pre-trained checkpoint\n",
    "# Note: Version 5 of the dataset \"csebuetnlp-1d1-2d12\" contains the best model. We have pinned that version. Kindly do not change the version to the lastest one.\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"/kaggle/input/csebuetnlp-1d1-2d12/bt5_on_3d1_2d1_final_prepro_xoxo\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"/kaggle/input/csebuetnlp-1d1-2d12/bt5_on_3d1_2d1_final_prepro_xoxo\")\n",
    "\n",
    "# Set the device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Loop through each sentence in the test dataset and generate the corresponding output\n",
    "index=1\n",
    "for sentence in tqdm(test_data['sentence']):\n",
    "    \n",
    "    # Format input with the sentence and desired prefix\n",
    "    input_str = sentence\n",
    "    \n",
    "    # Tokenize the input\n",
    "    input_ids = tokenizer.encode(input_str, return_tensors='pt').to(device)\n",
    "\n",
    "    # Generate the output\n",
    "    output_ids = model.generate(input_ids=input_ids, \n",
    "                                max_length=512, \n",
    "                                num_beams=4, \n",
    "                                early_stopping=True)\n",
    "\n",
    "    # Decode the output and remove unwanted tokens\n",
    "    output_str = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Add the transcription to the dataframe 'df'\n",
    "    data = {'Id': index,\n",
    "            'Expected': output_str}\n",
    "    df = df.append(data, ignore_index=True)\n",
    "    index=index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69b08b0",
   "metadata": {
    "papermill": {
     "duration": 0.605787,
     "end_time": "2023-03-18T04:29:17.177424",
     "exception": false,
     "start_time": "2023-03-18T04:29:16.571637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Post Processing(Takes only 5 sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d405cbfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T04:29:18.671968Z",
     "iopub.status.busy": "2023-03-18T04:29:18.671486Z",
     "iopub.status.idle": "2023-03-18T04:29:18.682424Z",
     "shell.execute_reply": "2023-03-18T04:29:18.681236Z"
    },
    "papermill": {
     "duration": 0.914851,
     "end_time": "2023-03-18T04:29:18.685594",
     "exception": false,
     "start_time": "2023-03-18T04:29:17.770743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_half_dollar_signs(s):\n",
    "    \"\"\"\n",
    "    This function takes a string s as input, splits it into words, \n",
    "    and replaces half of the dollar signs in each word with an asterisk. \n",
    "    The modified words are then joined back into a string and returned.\n",
    "    \"\"\"\n",
    "    def contains_only_dollar_signs(s):\n",
    "        for char in s:\n",
    "            if char != '$':\n",
    "                return False\n",
    "        return True  \n",
    "    \n",
    "    s2 = s.split()\n",
    "    for i in range(len(s2)):\n",
    "        if contains_only_dollar_signs(s2[i]):\n",
    "            dollar_count = s2[i].count('$')\n",
    "            s2[i] = s2[i][:(dollar_count+1)//2] + '*' + s2[i][(dollar_count+1)//2:]\n",
    "    return \" \".join(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e387aedd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T04:29:20.507314Z",
     "iopub.status.busy": "2023-03-18T04:29:20.506820Z",
     "iopub.status.idle": "2023-03-18T04:29:20.515540Z",
     "shell.execute_reply": "2023-03-18T04:29:20.514134Z"
    },
    "papermill": {
     "duration": 0.824688,
     "end_time": "2023-03-18T04:29:20.517940",
     "exception": false,
     "start_time": "2023-03-18T04:29:19.693252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def replace_double_spaces(str1, str2):\n",
    "    \"\"\"\n",
    "    Replaces double spaces in the first string with a '$ $' separator and replaces certain characters in the second string.\n",
    "\n",
    "    Args:\n",
    "    - str1: A string containing double spaces that need to be replaced.\n",
    "    - str2: A string containing characters that need to be replaced.\n",
    "\n",
    "    Returns:\n",
    "    - A new string with the specified modifications.\n",
    "\n",
    "    \"\"\"\n",
    "    # split str1 by double spaces and str2 by single spaces\n",
    "    str1_list = str1.split(\"  \")\n",
    "    str2_list = str2.split()\n",
    "    str2_list_copy = str2_list.copy()\n",
    "    \n",
    "    # Replace any occurrence of '$' in str2_list with an empty string\n",
    "    str2_list = [i.replace('$', '') for i in str2_list] \n",
    "    \n",
    "    # Split the first part of str1 by single spaces\n",
    "    str1_list_part1 = str1_list[0].split()  \n",
    "    \n",
    "    # Iterate through the split string, checking if each element is equal to the corresponding element in str2_list\n",
    "    for i in range(len(str1_list_part1)):\n",
    "        if (str1_list_part1[i] == str2_list[i]):\n",
    "            continue\n",
    "            \n",
    "    # Join the modified str2_list and add a '$ $' separator where the double spaces were in str1\n",
    "    output = \" \".join(str2_list_copy[:i+1]) + \"$ $ \" + \" \".join(str2_list_copy[i+1:])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ddd01a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T04:29:21.713973Z",
     "iopub.status.busy": "2023-03-18T04:29:21.713548Z",
     "iopub.status.idle": "2023-03-18T04:29:21.727477Z",
     "shell.execute_reply": "2023-03-18T04:29:21.726251Z"
    },
    "papermill": {
     "duration": 0.603224,
     "end_time": "2023-03-18T04:29:21.729989",
     "exception": false,
     "start_time": "2023-03-18T04:29:21.126765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def align(first_sentence, second_sentence):\n",
    "    \"\"\"\n",
    "    Aligns two given sentences by replacing words in the second sentence with words in the first sentence.\n",
    "\n",
    "    Args:\n",
    "    - first_sentence: A string representing the first sentence with dollar signs ($).\n",
    "    - second_sentence: A string representing the second sentence (original).\n",
    "\n",
    "    Returns:\n",
    "    - A new string with the second sentence aligned to the first sentence.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        first_sentence=replace_half_dollar_signs(first_sentence)\n",
    "        fs_words = first_sentence.split()\n",
    "        ss_words = second_sentence.split()\n",
    "        for i, (f_word, s_word) in enumerate(zip(fs_words, ss_words)):\n",
    "            if f_word == s_word:\n",
    "                continue\n",
    "            else:\n",
    "                index=0\n",
    "                count_head=0\n",
    "                count_tail=0\n",
    "                while f_word[index:index+1]=='$':\n",
    "                    count_head+=1\n",
    "                    index+=1\n",
    "                index=-1\n",
    "                if f_word[index]=='$':\n",
    "                    while f_word[index]=='$':\n",
    "                        count_tail+=1\n",
    "                        index-=1\n",
    "                    ss_words[i] = ('$'*count_head) + ss_words[i] + ('$'*count_tail)\n",
    "                else:\n",
    "                    things_to_ignore=[\"$,\",\"$?\",\"$!\",\"$।\",\"$'\"]\n",
    "                    found=False\n",
    "                    for j in things_to_ignore:\n",
    "                        if f_word == j:\n",
    "                           found=True\n",
    "                    if found == False:\n",
    "                        index=-2\n",
    "                        while f_word[index]=='$':\n",
    "                            count_tail+=1\n",
    "                            index-=1\n",
    "                        ss_words[i] = ('$'*count_head) + ss_words[i][:-1] + ('$'*count_tail) + ss_words[i][-1]\n",
    "                    else:\n",
    "                        ss_words[i] = ('$'*count_head) + ss_words[i]\n",
    "        joined_sentence = \" \".join(ss_words)\n",
    "        str_1=joined_sentence.replace(\"$\", \"\")\n",
    "        if len(str_1)!=len(second_sentence):\n",
    "            if second_sentence.count(\"  \") == 1:\n",
    "                first=second_sentence\n",
    "                second=joined_sentence\n",
    "                output = replace_double_spaces(first, second)\n",
    "                joined_sentence=output\n",
    "        return joined_sentence\n",
    "    except Exception as e:\n",
    "        return first_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ece30e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T04:29:23.018170Z",
     "iopub.status.busy": "2023-03-18T04:29:23.017754Z",
     "iopub.status.idle": "2023-03-18T04:29:27.311191Z",
     "shell.execute_reply": "2023-03-18T04:29:27.309908Z"
    },
    "papermill": {
     "duration": 4.896548,
     "end_time": "2023-03-18T04:29:27.315155",
     "exception": false,
     "start_time": "2023-03-18T04:29:22.418607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:04<00:00, 2335.88it/s]\n"
     ]
    }
   ],
   "source": [
    "index=0\n",
    "for i in tqdm(df[\"Expected\"]):\n",
    "    if i.find(\"$\")!=-1:\n",
    "        first_sentence = i\n",
    "        second_sentence = test_data[\"text\"][index]\n",
    "        joined_sentence=align(first_sentence,second_sentence)\n",
    "        df[\"Expected\"][index]=joined_sentence\n",
    "        index=index+1\n",
    "    else:\n",
    "        df[\"Expected\"][index]=test_data[\"text\"][index]\n",
    "        index=index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa4aa889",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T04:29:28.569973Z",
     "iopub.status.busy": "2023-03-18T04:29:28.569479Z",
     "iopub.status.idle": "2023-03-18T04:29:43.234559Z",
     "shell.execute_reply": "2023-03-18T04:29:43.233361Z"
    },
    "papermill": {
     "duration": 15.267951,
     "end_time": "2023-03-18T04:29:43.237514",
     "exception": false,
     "start_time": "2023-03-18T04:29:27.969563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if \" $।$\" in row[\"Expected\"]:\n",
    "        df.at[index, \"Expected\"] = row[\"Expected\"].replace(\" $।$\",\"$ $।\")\n",
    "    if \"$ ।$\" in row[\"Expected\"]:\n",
    "        df.at[index, \"Expected\"] = row[\"Expected\"].replace(\"$ ।$\",\"$ $।\")\n",
    "    if \"  ।\" in row[\"Expected\"]:\n",
    "        df.at[index, \"Expected\"]=row[\"Expected\"].replace(\"  ।\",\"$ $$ $।\")\n",
    "    if \"  ?\" in row[\"Expected\"]:\n",
    "        df.at[index, \"Expected\"]=row[\"Expected\"].replace(\"  ।\",\"$ $$ $।\")\n",
    "    if \"  !\" in row[\"Expected\"]:\n",
    "        df.at[index, \"Expected\"]=row[\"Expected\"].replace(\"  ।\",\"$ $$ $।\")\n",
    "    if \" ।\" in row[\"Expected\"]:\n",
    "        df.at[index, \"Expected\"] = row[\"Expected\"].replace(\" ।\",\"$ $।\")\n",
    "    if \" ?\" in row[\"Expected\"]:\n",
    "        df.at[index, \"Expected\"] = row[\"Expected\"].replace(\" ?\",\"$ $?\")\n",
    "    if \" !\" in row[\"Expected\"]:\n",
    "        df.at[index, \"Expected\"] = row[\"Expected\"].replace(\" !\",\"$ $!\")\n",
    "        \n",
    "for index, row in df.iterrows():\n",
    "    if '$* ' in row[\"Expected\"]:\n",
    "        a=row[\"Expected\"].replace(\"$* \",\"$\")\n",
    "        df[\"Expected\"][index]=align(a,test_data[\"text\"][index])\n",
    "for index, row in df.iterrows():\n",
    "    if test_data[\"text\"][index][0]==\" \" and row[\"Expected\"][0] != \"$\":\n",
    "        df[\"Expected\"][index]=\"$ $\" + row[\"Expected\"]\n",
    "        \n",
    "def append_if_not_match(string, patterns):\n",
    "    \"\"\"\n",
    "    Appends \"$$\" to the end of a string if the last two characters do not match\n",
    "    any of the specified patterns.\n",
    "    \"\"\"\n",
    "    status = True\n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, string[-2:]):\n",
    "            status = False\n",
    "            break\n",
    "    if status:\n",
    "        string += \"$$\"\n",
    "    return string\n",
    "\n",
    "patterns2 = [\"\\\\$\\\\$\", \"!\\\\$\", \"\\\\?\\\\$\", \"\\\\$\\\\$\", \"।\\\\$\", \"!\", \"\\\\?\",\"।\"]\n",
    "for index, row in df.iterrows():\n",
    "        string2=df[\"Expected\"][index]\n",
    "        df[\"Expected\"][index] = append_if_not_match(string2, patterns2)\n",
    "for index, row in test_data.iterrows():\n",
    "    if row[\"text\"][-1] == \" \":\n",
    "        df[\"Expected\"][index]=df[\"Expected\"][index]+\" \"\n",
    "for index, row in df.iterrows():\n",
    "    if '!$$' in row[\"Expected\"]:\n",
    "        df[\"Expected\"][index]=row[\"Expected\"].replace(\"!$$\",\"!\")\n",
    "    if '?$$' in row[\"Expected\"]:\n",
    "        df[\"Expected\"][index]=row[\"Expected\"].replace(\"?$$\",\"?\")\n",
    "    if '।$$' in row[\"Expected\"]:\n",
    "        df[\"Expected\"][index]=row[\"Expected\"].replace(\"।$$\",\"।\")\n",
    "i=False\n",
    "for ind, row in df.iterrows():\n",
    "    sentence=row[\"Expected\"].split()\n",
    "    for j in range(len(sentence)):\n",
    "        if \"!$\" in sentence[j]:\n",
    "            index=sentence[j].index(\"!\")\n",
    "            if sentence[j][index-1] != \"$\":\n",
    "                sentence[j]=sentence[j][:index]+\"$\"+sentence[j][index:]\n",
    "                i=True\n",
    "                if sentence[j][0]==\"$\":\n",
    "                    sentence[j]=sentence[j][:index]+\"$\"+sentence[j][index:]\n",
    "                    i=i+1\n",
    "    df[\"Expected\"][ind]=\" \".join(sentence)\n",
    "    if i==True:\n",
    "        i=False\n",
    "i=False\n",
    "for ind, row in df.iterrows():\n",
    "    sentence=row[\"Expected\"].split()\n",
    "    for j in range(len(sentence)):\n",
    "        if \"?$\" in sentence[j]:\n",
    "            index=sentence[j].index(\"?\")\n",
    "            if sentence[j][index-1] != \"$\":\n",
    "                sentence[j]=sentence[j][:index]+\"$\"+sentence[j][index:]\n",
    "                i=True\n",
    "                if sentence[j][0]==\"$\":\n",
    "                    sentence[j]=sentence[j][:index]+\"$\"+sentence[j][index:]\n",
    "                    i=i+1\n",
    "    df[\"Expected\"][ind]=\" \".join(sentence)\n",
    "    if i==True:\n",
    "        i=False\n",
    "i=False\n",
    "for ind, row in df.iterrows():\n",
    "    sentence=row[\"Expected\"].split()\n",
    "    for j in range(len(sentence)):\n",
    "        if \"।$\" in sentence[j]:\n",
    "            index=sentence[j].index(\"।\")\n",
    "            if sentence[j][index-1] != \"$\":\n",
    "                sentence[j]=sentence[j][:index]+\"$\"+sentence[j][index:]\n",
    "                i=True\n",
    "                if sentence[j][0]==\"$\":\n",
    "                    sentence[j]=sentence[j][:index]+\"$\"+sentence[j][index:]\n",
    "                    i=i+1\n",
    "    df[\"Expected\"][ind]=\" \".join(sentence)\n",
    "    if i==True:\n",
    "        i=False\n",
    "i=False\n",
    "for ind, row in df.iterrows():\n",
    "    if \",$ $\" not in sentence:\n",
    "        sentence=row[\"Expected\"].split()\n",
    "        for j in range(len(sentence)):\n",
    "            if \",$\" in sentence[j]:\n",
    "                index=sentence[j].index(\",\")\n",
    "                if sentence[j][index-1] != \"$\":\n",
    "                    sentence[j]=sentence[j][:index]+\"$\"+sentence[j][index:]\n",
    "                    i=True\n",
    "                    if sentence[j][0]==\"$\":\n",
    "                        sentence[j]=sentence[j][:index]+\"$\"+sentence[j][index:]\n",
    "                        i=i+1\n",
    "        df[\"Expected\"][ind]=\" \".join(sentence)\n",
    "        if i==True:\n",
    "            i=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3985bb3",
   "metadata": {
    "papermill": {
     "duration": 0.659025,
     "end_time": "2023-03-18T04:29:44.488809",
     "exception": false,
     "start_time": "2023-03-18T04:29:43.829784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Converting Dataframe to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a030c77c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T04:29:45.748490Z",
     "iopub.status.busy": "2023-03-18T04:29:45.748072Z",
     "iopub.status.idle": "2023-03-18T04:29:45.776478Z",
     "shell.execute_reply": "2023-03-18T04:29:45.775014Z"
    },
    "papermill": {
     "duration": 0.690302,
     "end_time": "2023-03-18T04:29:45.778917",
     "exception": false,
     "start_time": "2023-03-18T04:29:45.088615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7073707370737073\n"
     ]
    }
   ],
   "source": [
    "distance = 0\n",
    "for x,y in zip(test_data.ged, df.Expected):\n",
    "    distance+=Levenshtein.distance(x, y)\n",
    "print(distance/len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a28907e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T04:29:46.969001Z",
     "iopub.status.busy": "2023-03-18T04:29:46.968556Z",
     "iopub.status.idle": "2023-03-18T04:29:47.003378Z",
     "shell.execute_reply": "2023-03-18T04:29:47.002123Z"
    },
    "papermill": {
     "duration": 0.632567,
     "end_time": "2023-03-18T04:29:47.006398",
     "exception": false,
     "start_time": "2023-03-18T04:29:46.373831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('./predicted_sentence.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa0562",
   "metadata": {
    "papermill": {
     "duration": 0.600259,
     "end_time": "2023-03-18T04:29:48.265595",
     "exception": false,
     "start_time": "2023-03-18T04:29:47.665336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2846.846424,
   "end_time": "2023-03-18T04:29:52.625211",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-18T03:42:25.778787",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
