{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" # **Install & Import dependencies**","metadata":{}},{"cell_type":"code","source":"# Install tqdm package using pip\n!pip install tqdm ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary packages\nfrom tqdm import tqdm # tqdm is used to show progress bar\nimport pandas as pd # pandas is used for data manipulation\nimport re # re is used for regular expressions\nimport os # os is used for operating system related functions\nimport torch # torch is used for building deep learning models\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW # Import T5 model and its related classes from transformers package\nfrom transformers.optimization import AdamW, get_linear_schedule_with_warmup # Import AdamW optimizer and its related functions from transformers package","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merge Train Datasets into single DataFrame","metadata":{}},{"cell_type":"code","source":"# Load the first dataset into a pandas DataFrame\ndf1 = pd.read_csv(\"/kaggle/input/bengali-ged/DataSetFold1_u.csv\")\n\n# Load the second dataset into a pandas DataFrame\ndf2 = pd.read_csv(\"/kaggle/input/bengali-ged/DataSetFold2.csv\")\n\n# Load the extra dataset into a pandas DataFrame\n#The extra data contains 30 samples which we wanted our model to learn while training\ndf3 = pd.read_csv(\"/kaggle/input/extra-data/Bhashabhrom - Sheet1 (1).csv\")\n\n# Rename the 'original' column in the extra dataset to 'sentence'\ndf3.rename(columns = {'original':'sentence'}, inplace = True)\n\n# Concatenate the three datasets into one DataFrame\ndf = pd.concat([df1, df2, df3])\n\n# Reset the index of the DataFrame\ndf = df.reset_index(drop=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"# Iterate over each row in the DataFrame\nfor index, row in df.iterrows():\n    \n    # Replace \" $।$\" with \"$ $।\" in the 'gt' column\n    if \" $।$\" in row[\"gt\"]:\n        df.at[index, \"gt\"] = row[\"gt\"].replace(\" $।$\",\"$ $।\")\n        \n    # Replace \"$ ।$\" with \"$ $।\" in the 'gt' column\n    if \"$ ।$\" in row[\"gt\"]:\n        df.at[index, \"gt\"] = row[\"gt\"].replace(\"$ ।$\",\"$ $।\")\n        \n    # Replace \" ।\" with \"$ $।\" in the 'gt' column\n    if \" ।\" in row[\"gt\"]:\n        df.at[index, \"gt\"] = row[\"gt\"].replace(\" ।\",\"$ $।\")\n        \n    # Replace \"  \" with \"$ $\" in the 'gt' column\n    if \"  \" in row[\"gt\"]:\n        df.at[index, \"gt\"]=row[\"gt\"].replace(\"  \",\"$ $ \")\n        \n    # Replace \" $,$\" with \"$ $,\" in the 'gt' column\n    if \" $,$\" in row[\"gt\"]:\n        df.at[index, \"gt\"]=row[\"gt\"].replace(\" $,$\",\"$ $,\")\n        \n    # Remove consecutive '$' characters after '।' in the 'gt' column5\n    if re.search(r\"।\\${3,}\", row[\"gt\"]):\n        df.at[index, \"gt\"] = re.sub(r\"\\${3,}\", \"$\", row[\"gt\"])\n        \n        \n    # Remove consecutive '$' characters after '!' in the 'gt' column\n    if re.search(r\"\\!(\\$){3}\", row[\"gt\"]):\n        df.at[index, \"gt\"] = re.sub(r\"\\${3,}\", \"$\", row[\"gt\"])\n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def append_if_not_match(string, patterns):\n    \"\"\"\n    Appends \"$$\" to the end of a string if the last two characters do not match\n    any of the specified patterns.\n    \"\"\"\n    status = True\n    for pattern in patterns:\n        if re.search(pattern, string[-2:]):\n            status = False\n            break\n    if status:\n        string += \"$$\"\n    return string\npatterns2 = [\"\\\\$\\\\$\", \"!\\\\$\", \"\\\\?\\\\$\", \"\\\\$\\\\$\", \"।\\\\$\", \"!\", \"\\\\?\",\"।\"]\nfor index, row in df.iterrows():\n        string2=row[\"gt\"]\n        df.at[index, \"gt\"] = append_if_not_match(string2, patterns2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Disabling logging of training metrics to the wandb servers.\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing the T5 tokenizer and model\nprey='/kaggle/input/csebuetnlp-1d1-2d12/bt5_on_3d1_2d1_final_prepro_xoxo'\ntokenizer = T5Tokenizer.from_pretrained(prey)  # load T5 tokenizer from pre-trained model\nmodel = T5ForConditionalGeneration.from_pretrained(prey)  # load T5 model for text-to-text conversion from pre-trained model\n\n# Setting the device to GPU if available, else CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # check if GPU is available, else use CPU\nmodel = model.to(device)  # move the model to GPU/CPU\n\n# Defining the training parameters\nEPOCHS = 100  # number of training epochs\nBATCH_SIZE = 16  # batch size for training\nLEARNING_RATE = 1e-4  # learning rate for optimizer\nWARMUP_STEPS = 100  # number of warmup steps for scheduler\nTOTAL_STEPS = (len(df) // BATCH_SIZE) * EPOCHS  # total number of training steps\n\n# Defining the optimizer and learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)  # create AdamW optimizer with specified learning rate\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=TOTAL_STEPS)  # create linear scheduler with specified number of warmup steps and total training steps\n\n# Training the model\nfor epoch in range(EPOCHS):\n    for i in tqdm(range(0, len(df), BATCH_SIZE)):\n        batch = df.iloc[i:i+BATCH_SIZE]  # get a batch of input data from the DataFrame\n        sentences = list(batch[\"sentence\"])  # extract input sentences from the batch\n        gts = list(batch[\"gt\"])  # extract corresponding ground truth output sentences from the batch\n        input_ids = tokenizer.batch_encode_plus(sentences, padding=True, return_tensors=\"pt\")[\"input_ids\"].to(device)  # tokenize and encode the input sentences, and move the resulting tensors to GPU/CPU\n        gt_ids = tokenizer.batch_encode_plus(gts, padding=True, return_tensors=\"pt\")[\"input_ids\"].to(device)  # tokenize and encode the ground truth output sentences, and move the resulting tensors to GPU/CPU\n        \n        model.train()  # set the model in training mode\n        optimizer.zero_grad()  # reset the gradients to zero\n        loss = model(input_ids=input_ids, labels=gt_ids).loss  # compute the loss for the current batch\n        loss.backward()  # compute the gradients\n        optimizer.step()  # update the model parameters\n        scheduler.step()  # update the learning rate schedule\n        # Print the loss every 100 batches\n        if i % (100 * BATCH_SIZE) == 0:\n            print(f\"Epoch {epoch}, Batch {i}: Loss {loss.item()}\")\n\n# Saving the trained model\njoker=\"bt5_on_3d1_2d2_1d1d2_final_prepro_xoxo\"\nmodel.save_pretrained(joker)  # save the trained T5 model to the specified directory\ntokenizer.save_pretrained(joker)  # save the trained tokenizer to the specified directory","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}